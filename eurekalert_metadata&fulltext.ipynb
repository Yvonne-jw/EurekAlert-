{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0e3e1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urlexpander\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lxml import etree\n",
    "from pandas.core.frame import DataFrame\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import bs4\n",
    "import math\n",
    "import pymssql\n",
    "import xml.etree.ElementTree as ET\n",
    "import os, time, random\n",
    "import threading\n",
    "from random import randint \n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79a8a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_header( ):\n",
    "    referer = 'https://www.eurekalert.org/'\n",
    "    headers = {\n",
    "\n",
    "        'referer': referer,\n",
    "        'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36',\n",
    "        'content-type': 'application/x-www-form-urlencoded',\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "\n",
    "        }\n",
    "    return headers\n",
    "\n",
    "\n",
    "def get_data(url):\n",
    "    '''\n",
    "    功能：访问 url 的网页，获取网页内容并返回\n",
    "    参数：\n",
    "        url ：目标网页的 url\n",
    "    返回：目标网页的 html 内容\n",
    "    '''\n",
    "    headers = get_header( )\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, headers=headers)\n",
    "        #r.encoding = \"utf-8-sig\"\n",
    "        r.raise_for_status()\n",
    "        return r.text\n",
    "    \n",
    "    #except requests.HTTPError as e:\n",
    "       # print(e)\n",
    "        #print(\"HTTPError\")\n",
    "    #except requests.RequestException as e:\n",
    "       # print(e)\n",
    "    except:\n",
    "        null = 0\n",
    "        return null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17cab2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse('C:/Users/zhangz10/ER/sitemap.xml')\n",
    "root = tree.getroot()\n",
    "urlmaps = []\n",
    "for elem in root:\n",
    "    li = []\n",
    "    for subelem in elem:\n",
    "        li.append(subelem.text)\n",
    "    urlmaps.append(li[0])\n",
    "urlmaps= urlmaps[0:len(urlmaps)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba9eb454",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "for url in urlmaps:\n",
    "    r = requests.get(url)\n",
    "    root = ET.fromstring(r.content)\n",
    "    for elem in root:\n",
    "        li = []\n",
    "        for subelem in elem:\n",
    "            li.append(subelem.text)\n",
    "        urls.append(li[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "1cd6d711",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for i in urls:\n",
    "    # get article id\n",
    "    j=i[i.rindex('ses')+4:]\n",
    "    d[j] = i\n",
    "df = pd.DataFrame(d.items())\n",
    "df.columns = ['euid', 'url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "eeee87c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfurl.to_csv(r'eurekalert_2103-2303.csv'\n",
    "          #, header=None\n",
    "          , index=None\n",
    "          , sep=';'\n",
    "          , encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "4b0256a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eurekalert url data\n",
    "f = open('dataset.txt')\n",
    "lines = f.readlines()\n",
    "l = []\n",
    "for line in lines:\n",
    "    li = []\n",
    "    euid = line.replace('\\n','').split('/')[2]\n",
    "    url = 'http://www.'+ line.replace('\\n','')\n",
    "    li = [euid,url]\n",
    "    l.append(li)\n",
    "dfurl = pd.DataFrame(l)\n",
    "dfurl.columns = ['euid','url']\n",
    "dfurl = pd.concat([dfurl,df],ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "677283f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eurekalert(euid,url):\n",
    "    r = requests.get(url)\n",
    "    html = r.content\n",
    "    soup = bs4.BeautifulSoup(html)\n",
    "    parse_html = etree.HTML(html)\n",
    "    \n",
    "#metadata\n",
    "    doi = title= description= keywords= funder= journal= category= institution= meeting= date= original_source= None None\n",
    "    \n",
    "    title = soup.find_all('meta', property=\"og:title\")[0]['content']\n",
    "    description = soup.find_all('meta', property=\"og:description\")[0]['content']\n",
    "    \n",
    "    k1 = soup.find_all('span',class_ =\"ea-keyword__short\")\n",
    "    k2 = soup.find_all('span',class_ =\"ea-keyword__path\" )\n",
    "    if k1:\n",
    "        keywords1 =''\n",
    "        keyword = []\n",
    "        for i in range(len(k1)):\n",
    "            keyword.append(k1[i].string)\n",
    "            if k2[i]:\n",
    "                if i == len(k1)-1:\n",
    "                    keyword.append(k2[i].string)\n",
    "                else:\n",
    "                    keyword.append(k2[i].string+',') \n",
    "        for j in keyword:\n",
    "            keywords1 += str(j)\n",
    "        keywords = keywords1\n",
    "    \n",
    "    jf1 = soup.find_all('dt')\n",
    "    jf2 = soup.find_all('dd')\n",
    "    for i in range(len(jf1)):\n",
    "        if 'DOI' in jf1[i]:\n",
    "            doi = jf2[i].string\n",
    "        if 'Journal' in jf1[i]:\n",
    "            journal = jf2[i].string\n",
    "        if 'Funder' in jf1[i]:\n",
    "            funder = jf2[i].string.replace('\\t','').replace('\\n','').replace('  ','')\n",
    "        if 'Meeting' in jf1[i]:\n",
    "            meeting = jf2[i].string.replace('\\t','').replace('\\n','').replace('  ','')\n",
    "            \n",
    "    category = soup.find_all('a',style=\"color:red;\")[0].string  \n",
    "    institution =soup.find_all('p',class_=\"meta_institute\")[0].string\n",
    "    #post time\n",
    "    date = soup.find_all('time')[0].string.replace(' ','').replace('\\n','')\n",
    "    if date == '':\n",
    "        date = parse_html.xpath('//*[@id=\"ahmain-content\"]/header/div[1]/text()')[0].split('|')[1].replace(' ','').replace('\\n','')\n",
    "   #original source \n",
    "    for i in range(1,5):\n",
    "        source = parse_html.xpath('//*[@id=\"sidebar-content\"]/section[2]/div['+str(i)+']/h4/text()')\n",
    "        if source and 'Original Source' in source1[0]:\n",
    "            source_url =parse_html.xpath('//*[@id=\"sidebar-content\"]/section[1]/div['+str(i)+']/a/text()')[0]\n",
    "\n",
    "    original_source = source_url.replace('\\t','').replace('\\n','').replace(' ','')\n",
    "\n",
    "    matrix= [[euid,url, doi, title, description, keywords, funder, journal, category, institution,meeting,date,original_source]]\n",
    "    file1 = open('D:/Users/zhangz10/ER/metadata_data.csv', 'a+', newline='', encoding='utf-8')\n",
    "    writer1= csv.writer(file1, delimiter=';')\n",
    "    writer1.writerows(matrix)\n",
    "    file1.close()\n",
    "    doi = title= description= keywords= funder= journal= category= institution= meeting= date=orginal_source= None None\n",
    "\n",
    " #full-text     \n",
    "    all_link= news= image= None\n",
    "    \n",
    "    content = soup.find_all('div',class_ = 'ahcenter')[0]\n",
    "    #content = soup.find_all('div',class_ = 'entry')[0]\n",
    "    new = content.find_all('p')\n",
    "    news1 = ''\n",
    "    for j in range(len(new)):\n",
    "        con = parse_html.xpath('//*[@id=\"ahmain-content\"]/div[1]/p['+str(j)+']/text()')\n",
    "        #con = parse_html.xpath('//*[@id=\"main-content\"]/article/div/p['+str(j)+']/text()')\n",
    "        if con:\n",
    "            news1 += str(con[0].replace('\\t','').replace('\\n','').replace('  ',''))+' '            #newscontent\n",
    "    news = news1\n",
    "    \n",
    "    link = content.find_all('a',href=True)\n",
    "    links = ''\n",
    "    for i in link:   #['href']#all url do for\n",
    "        if 'http' in i['href']:\n",
    "            links += str(i['href'])+ ','      #alllink\n",
    "    all_link = links\n",
    "    \n",
    "    images = ''\n",
    "    for image in soup.find_all('figure'):\n",
    "       \n",
    "        if image.find_all('a'):\n",
    "            image_id = image.find_all('a')[0]['href']  #//*[@id=\"main-content\"]/article/div[1]/figure/figcaption/p[1]/strong/text()\n",
    "            image_caption = image.find_all('strong')[0].string #image.find_all('img')[0]['title']#\n",
    "            if image.find_all('img'):\n",
    "                image_url = image.find_all('img')[0]['src']\n",
    "            else:\n",
    "                image_url = None\n",
    "            images += str(image_id)+','\n",
    "            picture = [[euid,image_id,image_caption,image_url]]\n",
    "            file3 = open('D:/Users/zhangz10/ER/newimage_twitter.csv', 'a+', newline='', encoding='utf-8')\n",
    "            writer3= csv.writer(file3, delimiter=';').writerows(picture)\n",
    "            file3.close()\n",
    "    all_images = images\n",
    "    \n",
    "    fulltext = [[euid,news,all_images,all_link]]\n",
    "    file2 = open('D:/Users/zhangz10/ER/newfulltext_twitter.csv', 'a+', newline='', encoding='utf-8')\n",
    "    writer2= csv.writer(file2, delimiter=';')\n",
    "    writer2.writerows(fulltext)\n",
    "    all_link= news= image= None\n",
    "    file2.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3b55f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpunum(data):\n",
    "        n_cpu = 10 # get CPU number\n",
    "        num = len(data) # 数据集样本数量\n",
    "        #data.n_cpu = n_cpu\n",
    "        #print('cpu num: {}'.format(n_cpu))\n",
    "        chunk_size = int(num / n_cpu) # 分摊到每个CPU上的样本数量\n",
    "        procs = []\n",
    "        for i in range(0, n_cpu):\n",
    "            min_i = chunk_size * i\n",
    "            if i < n_cpu - 1:\n",
    "                max_i = chunk_size * (i + 1)\n",
    "            else:\n",
    "                max_i = num\n",
    "            digits = [data[min_i: max_i],i,min_i,max_i]\n",
    "            procs.append(digits) \n",
    "            #t = threading.Thread(target=process, args=(digits[0],digits[2],digits[3],\"parallel\"))\n",
    "            #t.start()\n",
    "        return procs\n",
    "\n",
    "def run(twdata):\n",
    "    s = time.time()\n",
    "    #q =Queue()\n",
    "    threads = []\n",
    "    m = cpunum(twdata)\n",
    "    for i in range(10):\n",
    "        t = threading.Thread(target=process, args=(m[i][0],m[i][2],m[i][3],m[i][1]))\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    #results = []\n",
    "    #for _ in range(20):\n",
    "   #     results += q.get()\n",
    "    t.join()\n",
    "    e = time.time()\n",
    "    print(\"\\n应用多进程耗时: %0.2f seconds\" % (e - s))\n",
    "    #return results\n",
    "\n",
    "def process(data,start,end,name):\n",
    "    print('执行任务%s (%s)...' % (name, (end-start)))\n",
    "    s = time.time()\n",
    "    l1 = []\n",
    "    for i in range(start,end):\n",
    "        euid = data['euid'][i]\n",
    "        url = 'http://www.eurekalert.org/news-releases/'+ str(euid)\n",
    "        try:\n",
    "            get_eurekalert_originalurl(euid,url)\n",
    "        except:\n",
    "            l1.append(data['euid'][i])\n",
    "    e = time.time()\n",
    "    print('任务 %s 运行了 %0.2f seconds.' % (name, (e - s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd266129",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(dfurl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
